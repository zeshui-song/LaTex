\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{amsmath, amsfonts, bm, graphicx}
\usepackage{float}
\geometry{margin=1in}
\title{}
\date{}
\author{}

\begin{document}
\begin{center}
    \Large \textbf{Differential Equations Review Sheet} \\
    \normalsize Exam II
\end{center}
\section*{4.1 Linear Equations}
General form of a nth order linear differential equation:
\[a_n(x)y^{(n)}+ a_{n-1}(x)y^{(n-1)}+\cdots+a_1(x)y'+a_0(x)y=g(x)\]
\underline{\textbf{Thm: 4.1.1 Existence of a Unique Solution:}}\\
For an interval $I: a<x<b$, if the functions $a_n(x), a_{n-1}(x), \ldots, a_0(x)$ and $g(x)$ are continuous on $I$ and $a_n(x)\neq 0$ for all $x$ in $I$, then there exists a unique solution $y=y(x)$ of the differential equation.\\\\
If $x=x_o$ is in $I$, then a solution that satisfies the initial conditions exists on the interval and is unique.\\
\underline{Note:} If $a_n(x)=0$ for some $x$ in the interval $I$ then there \textbf{may not} exist a unique solution.\\\\
\textbf{Boundary Value Problems}\\
If the constraints of a linear differential equation are at \textit{different points} instead of using derivatives at the \textit{same} point then it is known as a \textbf{boundary value problem (BVP)} and the constraints are known as \textbf{boundary conditions}.\\\\
\textbf{Homogeneous vs Nonhomogeneous nth order ODE}\\
Homogeneous: $a_n(x)y^{(n)}+ a_{n-1}(x)y^{(n-1)}+\cdots+a_1(x)y'+a_0(x)y=0$\\\\
Nonhomogeneous: $a_n(x)y^{(n)}+ a_{n-1}(x)y^{(n-1)}+\cdots+a_1(x)y'+a_0(x)y=g(x)$, where $g(x)\neq 0$\\\\
\underline{\textbf{Thm 4.1.2 Superposition Principle - Homogeneous Equations}}\\
If $y_1, y_2,\ldots, y_k$ are solutions to the \textit{homogeneous} nth-order linear differential equation on an interval $I$, then the linear combination where $c_1, c_2, \ldots, c_n$ are arbitrary constants.
\[y =c_1 y_1(x)+ c_2y_2(x)+\cdots+c_k y_k(x)\]
is \textit{also} a solution on the interval.\\\\
\newpage\noindent
\textbf{Fundamental Set of Solutions}\\
There exists a fundamental set of solutions $\{y_1, y_2, \ldots, y_n\}$ to the homogeneous nth-order linear differential equation. The fundamental set of solutions has to be \textit{linearly independent}.\\\\
We test for linear independence using the \textbf{Wronskian}:\\\\
The solutions $\{y_1, y_2, \ldots, y_n\}$ are linearly independent on the interval $I$ if and only if
\[W(y_1, y_2,..., y_n)=\begin{vmatrix}
y_1&y_2&\cdots &y_n\\
Dy_1&Dy_2&\cdots &Dy_n\\
\vdots&\vdots&&\vdots\\
D^{n-1}y_1&D^{n-1}y_2&\cdots &D^{n-1}y_n\\
\end{vmatrix}\neq0\]
for every $x$ in the interval.\\\\
\underline{Note:} $D$ is the differentiation operator, i.e. $Dy=\frac{dy}{dx}$, $D^2y=\frac{d^2y}{dx^2}$, and so on.\\\\
\underline{\textbf{Thm 4.1.5 General Solution — Homogeneous Equations}}\\
If $y_1, y_2, \ldots, y_n$ is a fundamental set of solutions to the homogeneous nth-order linear differential equation on an interval $I$, then the general solution is given by
\[y=c_1 y_1(x)+ c_2y_2(x)+\cdots+c_n y_n(x)\]
where $c_1, c_2, \ldots, c_n$ are arbitrary constants.\\\\
\underline{\textbf{Thm 4.1.6 General Solution — Nonhomogeneous Equations}}\\
If $y_p$ is a particular solution to the nonhomogeneous nth-order linear differential equation on an interval $I$, and if $y_h$ is the general solution to the corresponding homogeneous equation, then the general solution to the nonhomogeneous equation is given by
\[y=y_h + y_p\]
\newpage\noindent
\section*{4.2 Reduction of Order}
We know that the general solution of a homogeneous linear second-order differential equation
\[a_2(x)y''+ a_1(x)y'+ a_0(x)y =0 \]
is a linear combination 
\[y = c_1y_1 +c_2y_2\]
where $y_1$ and $y_2$ are solutions that constitute a linearly independent set on some interval $I$.\\\\
Given that the differential equation only has constant coefficients, if we know one solution $y_1$, we can find a second solution $y_2$ using the method of \textbf{reduction of order}.\\\\
\textbf{Method:}\\
Since $y_1$ and $y_2$ are linearly independent, then their quotient $y_2/y_1$ hat to be nonconstant on $I$. In other words,
\[\frac{y_2(x)}{y_1(x)}=u(x)\]
\[\implies y_2(x) = u(x) y_1(x)\]
We can find the function $u(x)$ by substituting $y_2(x) = u(x) y_1(x)$ into the original differential equation and solving for $u(x)$.\\\\
\underline{Note:} This method reduces the differential equation from second-order to first-order in terms of $w=u'(x)$. But, it only works for second order ODE.\\\\
\textbf{Solving for $u(x)$ for a General Case:}\\
For a second-order linear differential equation with constant coefficients:
\[a_2(x)y''+a_1(x)y'+a_0(x)y=0\]
We put into general form by dividing through by $a_2(x)$:
\[y''+P(x)y'+Q(x)y=0\]
Assuming we know one solution $y_1$, we let $y_2 = u(x)y_1(x)$. Then we compute the first and second derivatives of $y_2$:
\[y_2' = u'y_1 + uy_1'\]
\[y_2'' = u''y_1 + 2u'y_1' + uy_1''\]
Substituting $y_2$, $y_2'$, and $y_2''$ into the original differential equation gives:
\[u\left[y_1'' + P(x)y_1' + Q(x)y_1\right]+ u''y_1 + u'(2y_1' + P(x)y_1) = 0\]
Since $y_1$ is a solution to the original equation, $y_1'' + P(x)y_1' + Q(x)y_1=0$.
\[\implies u''y_1 + u'(2y_1' + P(x)y_1) = 0\]
Letting $w = u'$, we have a linear first order differential equation in $w$:
\[w'y_1 + w(2y_1' + P(x)y_1) = 0\]
We can use an integrating factor to solve for $w$:\\\\
\fbox{%
\begin{minipage}{0.95\textwidth}
\underline{Recall:} Given a first-order linear ODE of the form
\[\frac{dy}{dx}+p(x)y=g(x)\]
the integrating factor is given by
\[\mu(x) = e^{\textstyle\int p(x) \, dx}\]
and the solution is 
\[y(x)=\frac{1}{\mu}\int \mu g(x) \, dx\]
\end{minipage}%
}\\\\
Thus,
\[w'+ w\frac{2y_1' + P(x)y_1}{y_1} = 0\]
\[\mu(x) = e^{\textstyle \int \frac{2y_1' + P(x)y_1}{y_1} \, dx}\]
\[\int \frac{2y_1' + P(x)y_1}{y_1} \, dx = \int 2 \frac{y_1'}{y_1} \,dx + \int P(x) \, dx, \quad u=y_1, du=y'_1dx\]
\[2\int \frac{1}{u}\,dx+\int P(x) \, dx\]
\[2ln(y_1)+\int P(x) \, dx\]
\[\implies \mu(x) = e^{2\ln(y_1) + \int P(x) \, dx} = y_1^2 e^{\int P(x) \, dx}\]
Finally, we can solve for $w$:
\[w(x) = \frac{1}{\mu(x)}\int \mu(x) \cdot 0 \, dx = \frac{c_1}{\mu(x)} = c_1\frac{e^{-\int P(x) \, dx}}{y_1^2}\]
Then, we can find $u(x)$ by integrating $w$:
\[u(x) = \int w(x) \, dx =  c_1\int\frac{e^{-\int P(x) \, dx}}{y_1^2} \, dx+c_2\]
Choosing $c_1=1$ and $c_2=0$ for the fundamental set:
\[\boxed{u(x)=\int\frac{e^{-\int P(x) \, dx}}{y_1^2} \, dx}\]
Where $y_2(x) = u(x)y_1(x)$.
\newpage\noindent
\section*{4.3 Homogeneous Linear Equations with Constant Coefficients}
For higher order homogeneous linear differential equations with constant coefficients of the form, we can use the characteristic equation to find the general solution.\\\\
\textbf{Method:}\\
Given a homogeneous linear differential equation with constant coefficients:
\[ay'' + by' + cy = 0\]
We assume the solution of the form:
\vspace{-1em}
\begin{eqnarray*}
y(t) &=& e^{rt}\\
y'(t) &=& re^{rt}\\
y''(t) &=& r^{2}e^{rt}\\
\end{eqnarray*}
Then,  
\vspace{-1em}
\begin{eqnarray*}
ay''+by'+cy&=&0\\
ar^{2}e^{rt}+bre^{rt}+ce^{rt}&=&0\\
e^{rt}(ar^{2}+br+c)&=&0\\
ar^{2}+br+c&=&0
\end{eqnarray*}
We can solve for $r$ using the quadratic formula:
\[r=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\]
\underline{Note:} For higher order differential equations, we would get a polynomial of degree $n$, and we would solve for the roots by factoring.\\\\
\textbf{Case 1: Real and Unique Roots}\\
For roots $r_1$ and $r_2$, the general solution is:
\[\boxed{y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t}}\]
\textbf{Case 2: Complex Roots}\\
For roots $\alpha \pm \beta i$, the general solution is:
\[\boxed{y(t) =   c_1 e^{\alpha t}\cos \beta t + c_2 e^{\alpha t} \sin \beta t}\]
\textbf{Case 3: Repeated Roots}\\
For repeated root $r$, the general solution is:
\[\boxed{y(t) = c_1 e^{r t} + c_2 t e^{r t}}\]
\newpage\noindent
\section*{4.4 Undetermined Coefficients (Superposition Approach)}
To find a particular solution $Y_p$ to the higher order nonhomogeneous differential equation with constant coefficients, we will guess the form of $Y_p$ based on the form of $g(x)$.\\\\
\textbf{Method:}
\begin{itemize}
    \item Guess a form of $Y_{P}(t)$ leaving the coefficient(s) undetermined (and hence the name of the method).
    \item Plug the guess into the differential equation and see if we can determine values of the coefficients.
    \item If we can determine values for the coefficients then we guessed correctly, if we can't find values for the coefficients then we guessed incorrectly.
\end{itemize}
In general, we will guess

\[\begin{array}{c|c}
g(t) & Y_{P}(t) \\
\hline\hline
ae^{bt} & Ae^{bt} \\
a\cos(\beta t) & A\cos(\beta t)+B\sin(\beta t) \\
a\sin(\beta t) & A\cos(\beta t)+B\sin(\beta t) \\
a\cos(\beta t)+b\sin(\beta t) & A\cos(\beta t)+B\sin(\beta t) \\
\text{n-th degree polynomial} & A_{n}t^{n}+\cdots +A_{1}t+A_{0}
\end{array}\]\\
\underline{Note:} If any term of $Y_{P}$ is duplicated in $y_{h}$ then the duplicated term must be multiplied by the minimum $+$ integer power of $t$ required to make all terms in general solution linearly independent. (So find the general homogeneous solution first before guessing the particular solution!)\\\\
Then we will substitute our guess for $Y_{P}(t)$ into the original differential equation and solve for the undetermined coefficients.
\section*{4.5 Undetermined Coefficients (Annihilator Approach)}
Another way to solve for a particular solution $Y_p$ to the higher order nonhomogeneous differential equation with constant coefficients is to use the \textbf{differential operator} $D$ to create an \textbf{annihilator operator} to reduce the ODE.\\\\
\textbf{Annihilator}\\
If $L$ is a differential operator such that 
\[L(g(x))=0\]
then $L$ is called an \textbf{annihilator} of $g(x)$.\\\\
\newpage\noindent
\underline{Ex:} 
\[y''+y'-6y=0\]
\[D^2y + Dy - 6y = 0\implies (D^2 + D - 6)y = 0\]
Here, the differential operator is $D^2 + D - 6$.\\\\
\textbf{Method:}\\
Given a nonhomogeneous differential equation:
\[a_n y^{(n)} + a_{n-1} y^{(n-1)} + \cdots + a_1 y' + a_0 y = g(x)\]
We can rewrite it using the differential operator $L$ on $y$ such that:
\[L(y) = g(x)\]
Next, we find an annihilator $L_1$ of $g(x)$ such that:
\[L_1(g(x)) = 0\]
Then, we apply $L_1$ to both sides of the equation:
\[L_1(L(y)) = L_1(g(x))\]
\[\implies L_1 L(y) = 0\]
So by solving the \textit{homogeneous higher-order equation} $L_1L(y)=0$ we can discover the form of a particular solution $y_p$ for the original \textit{nonhomogeneous equation} $L(y)=g(x)$.\\\\
\textbf{List of Annihilators:}
\begin{itemize}
    \item $D^n$ annihilates polynomials of degree $n-1$ or less. Thus, it annihilates each of the following:
    \[1,x,x^2,\ldots,x^{n-1}\]
    \item $(D-\alpha)^n$ annihilates each of the following:
    \[e^{\alpha x},xe^{\alpha x},x^2e^{\alpha x},\ldots,x^{n-1}e^{\alpha x}\]
    \item $[D^2-2\alpha D+(\alpha^2+\beta^2)]^n$ annihilates each of the following:
    \[e^{\alpha x}\cos \beta x, xe^{\alpha x}\cos \beta x, \ldots, x^{n-1}e^{\alpha x}\cos \beta x\]
    \[e^{\alpha x}\sin \beta x, xe^{\alpha x}\sin \beta x, \ldots, x^{n-1}e^{\alpha x}\sin \beta x\]
\end{itemize}
\newpage\noindent
\underline{Ex:} Finding the particular solution for $y''+y'-6y=e^{4t}$\\
Using $y=e^{rt}$, we find the homogeneous solution:
\[y_h=c_1 e^{-3t} + c_2 e^{2t}\]
The annihilator for $e^{4t}$ is $(D-4)$. Thus, we apply the annihilator to both sides of the equation:
\[(D-4)(D^2 + D - 6)y = 0\]
Solving the characteristic equation:
\[(D-4)(D-2)(D+3)y=0\]
The general solution to the higher order homogeneous equation is (matching each annihilator factor to its corresponding $g(t)$ term):
\[y = c_1 e^{4t} + c_2 e^{2t} + c_3 e^{-3t}\]
Thus, we can choose the particular solution to be:
\[y_p = A e^{4t}\]
Substituting $y_p$ into the original differential equation we can solve for $A$.
\section*{4.6 Variation of Parameters}
For nonhomogeneous linear differential equations where the method of undetermined coefficients does NOT work (ie. $g(x)$ is not of the right form, and NONconstant coefficients), we can use the method of \textbf{variation of parameters} to find a particular solution.\\\\
\underline{Note:} Variation of parameters works for higher order ODEs, but the formulas get more complicated so most examples will be second order ODEs. \\
IF we are given $y_h$, the homogeneous solution, we can find $y_p$, the particular solution, using variation of parameters. However, if we are not given $y_h$, we must find it first using the characteristic equation method. (Thus, unless given $y_h$ the problems we do will still only be constant coefficients)\\\\
\textbf{Method:}\\
\vspace{-1em}
\begin{itemize}
    \item Obtain general homogeneous solution:
    \[y_h=c_1y_1 + c_2 y_2\]
    For the second order ODE:
    \[y'' + p(x)y' + q(x)y = g(x)\]
    \underline{Note:} the coefficient of $y''$ \textbf{must} be 1, so divide through if necessary.
    \item The particular solution is of the form:
    \[y_p = u_1(x)y_1 + u_2(x)y_2\]
    where we need to solve for $u_1(x)$ and $u_2(x)$.
\end{itemize}
Where $W(y_1,y_2)$ is the Wronskian of $y_1$ and $y_2$:
\[\boxed{{u_1=-\int{\frac{y_2g(t)}{W(y_{1},y_{2})}} \ dt, \ \  u_2=\int{\frac{y_1g(t)}{W(y_{1},y_{2})}} \ dt}}\]
Finally, the particular solution is:
\[\boxed{Y_{P} = -y_{1}\int{\frac{y_2g(t)}{W(y_{1},y_{2})}} \ dt+y_{2}\int{\frac{y_1g(t)}{W(y_{1},y_{2})}} \ dt}\]
\section*{4.7 Cauchy-Euler Equation}
Nonhomogeneous differential equations with variable coefficients of the form:
\[a_nx^ny^{(n)}+ a_{n-1}x^{n-1}y^{(n-1)}+\cdots+a_1xy'+a_0y=g(x)\]
where $a_n,a_{n-1},\ldots, a_1, a_0$ are constants, are known as \textbf{Cauchy-Euler equations}.\\\\
The method for solving Cauchy-Euler equations can be applied to higher order ODEs, but we will focus on second order ODEs.
\[ax^2y''+ bxy'+cy=g(x)\]
\underline{Note:} The coefficient $ax^2$ is zero at $x=0$.  To guarantee uniqueness we will assume we are looking for general solutions defined on the interval $(0,\infty)$.\\\\
\textbf{Method to solve for homogeneous solution:}\\
Given:
\[ax^2y''+ bxy'+cy=0\]
We assume the solution of the form:
\vspace{-1em}
\begin{eqnarray*}
y(t) &=& x^m\\
y'(t) &=& mx^{m-1}\\
y''(t) &=& m(m-1)x^{m-2}\\
\end{eqnarray*}
Plugging into the differential equation:
\vspace{-1em}
\begin{eqnarray*}
ax^2y''+ bxy'+cy&=&0\\
ax^2m(m-1)x^{m-2} + bxmx^{m-1} + cx^m &=& 0\\
x^m (am(m-1) + bm + c) &=& 0\\
am(m-1) + bm + c &=& 0
\end{eqnarray*}
Thus, the characteristic equation for $m$ is:
\[am^2 + (b-a)m + c = 0\]
\newpage\noindent
\textbf{Case 1: Real and Unique Roots}\\
For roots $m_1$ and $m_2$, the general solution is:
\[\boxed{y_h = c_1 x^{m_1} + c_2 x^{m_2}}\]
\textbf{Case 2: Repeated Roots}\\
If we have a repeated root then we know the discriminant of the quadratic formula must be zero.  Thus, the root is of the form $m_1=-(b-a)/(2a)$  To get the second solution we can do a \textbf{reduction of order}.\\\\
Rewriting the Cauchy-Euler equation in standard form:
\[y'' + \frac{b}{ax}y' + \frac{c}{ax^2}y = 0\]
Letting $y_1 = x^{m_1} \implies y_2 = u(x)y_1$.\\\\
Recall that:
\[u(x) = \int \frac{e^{-\int P(x) \,dx}}{y_1^2} \, dx\]
Where $P(x) = \frac{b}{ax}$ and
\[\int P(x) \, dx = \int \frac{b}{ax} \, dx = \frac{b}{a} \ln(x)\]  
Thus,
\begin{align*}
y_2
&= x^{m_1}\int\frac{e^{-b/a \ln(x)}}{(x^{m_1})^2} \, dx\\
&= x^{m_1}\int x^{-b/a} \cdot x^{-2m_1} \, dx\\
&= x^{m_1}\int x^{-b/a} \cdot x^{(b-a)/a} \, dx \qquad \leftarrow m_1 = -\frac{b-a}{2a}\\
&= x^{m_1}\int x^{-1} \, dx\\
&= x^{m_1} \ln(x)
\end{align*}
The general solution is then:
\[\boxed{y_h=c_1 x^{m_1} + c_2 x^{m_1} \ln(x)}\]
\newpage\noindent
\textbf{Case 3: Complex Roots}\\
If the roots are complex then they come in conjugate pairs $$m=\alpha\pm i \beta$$ so then a homogeneous solution is
\[y_h=c_1x^{\alpha+ i \beta}+c_2x^{\alpha- i \beta}\]
But then this solution is complex.  To fix first note the identity 
\[x^{i\beta}=(e^{\ln x})^{i\beta}=(e^{i\beta\ln x})\] 
and by Euler's formula
\[x^{i\beta}=\cos(\beta\ln x)+i\sin(\beta\ln x)\]
\[x^{-i\beta}=\cos(\beta\ln x)-i\sin(\beta\ln x)\]
So adding and subtracting the two gives
\[y_1=x^{i\beta}+x^{-i\beta}=2\cos(\beta\ln x)\]
\[y_2=x^{i\beta}-x^{-i\beta}=2i\sin(\beta\ln x)\]
But since we are dealing with just real solutions we can drop the $i$ in $y_2$ by assuming it is integrated into the constant $c_2$.  Thus the homogeneous solution is
\[\boxed{y_h=c_1x^{\alpha}\cos(\beta\ln x)+c_2x^{\alpha}\sin(\beta\ln x)}\] 
\underbar{Note:} In general, to find the particular solution $y_p$ for Cauchy-Euler equations, method of undetermined coefficients \textbf{does not} apply since the coefficients are not constant.  Thus, we must use variation of parameters to find $y_p$ using the homogeneous solution $y_h$.
\newpage
\section*{Eigenvalues and Eigenvectors}
For a matrix $\mathbf A\in\mathcal{R}^{n\times n}$. An element $\mathbf x\in\mathcal{R}^n$, where $\mathbf x\neq 0$ is called an \textbf{eigenvector} of $\mathbf A$ if there exists a scalar $\lambda$, known as an \textbf{eigenvalue} such that
\[\mathbf A\mathbf x = \lambda \mathbf x\]
In order to calculate the eigenvalues/vectors note that if \begin{eqnarray*}
\mathbf A\mathbf x &=& \lambda \mathbf x\\
\mathbf A\mathbf x - \lambda \mathbf x&=&\mathbf{O}\\
(\mathbf A - \lambda \mathbf{I})\mathbf x&=&\mathbf{O}
\end{eqnarray*}
Since $\mathbf x \neq \mathbf O$ then $\mathbf A-\lambda \mathbf I$ must \textbf{not} be invertible. Thus, 
\[\boxed{\det(\mathbf A-\lambda \mathbf I)=0}\]
\textbf{Method:}
\begin{itemize}
    \item Calculate the \textbf{characteristic polynomial} by looking for the eigenvalues $\lambda$ where $$\textrm{det}(\mathbf A-\lambda \mathbf I)=0$$
    \item Calculate the \textbf{eigenspace} associated with $\lambda$ by looking at $$\textrm{null}(\mathbf A-\lambda \mathbf I)$$
Any non-zero $\mathbf{x}\in\textrm{null}(\mathbf A-\lambda \mathbf I)$ is an \textbf{eigenvector} of $\mathbf A$ associated with $\lambda$.\\\\
\underline{Note:} The null space is all vectors $\mathbf x$ such that $(\mathbf A-\lambda \mathbf I)\mathbf x=\mathbf 0$.
\end{itemize}
\textbf{Defective Matrix}\\
A matrix $\mathbf A$ is said to be \textbf{defective} if it cannot be diagonalized. This occurs when there are not enough linearly independent eigenvectors to form a basis for $\mathcal{R}^n$.\\\\
We can check if a matrix is defective by comparing the \textbf{algebraic multiplicity} and \textbf{geometric multiplicity} of each eigenvalue.
\begin{itemize}
    \item \textbf{Algebraic Multiplicity:} The algebraic multiplicity of an eigenvalue $\lambda$ is the number of times $\lambda$ appears as a root of the characteristic polynomial.
    \item \textbf{Geometric Multiplicity:} The geometric multiplicity of an eigenvalue $\lambda$ is the dimension of the eigenspace associated with $\lambda$, which is given the by number of linearly independent eigenvectors associated with $\lambda$.
\end{itemize}
\underline{Note:} For characteristic polynomials with complex roots the eigenvalues will \textbf{always} come in pairs: $a\pm i b$.  The eigenvectors will also come in pairs $\mathbf x, \overline{\mathbf{x}}$. So we can solve for just the plus case and get the minus case by taking the complex conjugate.\\\\
\section*{Diagonalization}
If a matrix $\mathbf A$ is diagonalizable (non-defective) then we can write it as:
\[\mathbf A = \mathbf C\mathbf D\mathbf C^{-1}\]
Where $\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n$ are the linearly independent eigenvectors of $\mathbf A$ and $\lambda_1, \lambda_2, \ldots, \lambda_n$ are the corresponding eigenvalues, then 
\[\mathbf{C} =
\begin{pmatrix}
\big| & \big| &        & \big| \\
\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \\
\big| & \big| &        & \big|
\end{pmatrix}
\quad \text{and} \quad
\mathbf{D} =
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
0 & 0 & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}\]
\textbf{Matrix Inverses}\\
For a 2x2 matrix
\[
A =
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}\]
\[A^{-1} = \frac{1}{ad - bc}
\begin{pmatrix}
d & -b \\
-c & a
\end{pmatrix},
\quad \text{if } ad - bc \neq 0.\]
For higher order matrices, we can use row reduction on the augmented matrix 
\[
\left[ 
\begin{array}{c|c}
\mathbf{A} & \mathbf{I} 
\end{array} 
\right]
=
\left[
\begin{array}{ccc|ccc}
a_{11} & a_{12} & a_{13} & 1 & 0 & 0 \\
a_{21} & a_{22} & a_{23} & 0 & 1 & 0 \\
a_{31} & a_{32} & a_{33} & 0 & 0 & 1
\end{array}
\right]
\;\xrightarrow{\text{row reduce}}\;
\left[
\begin{array}{ccc|ccc}
1 & 0 & 0 & b_{11} & b_{12} & b_{13} \\
0 & 1 & 0 & b_{21} & b_{22} & b_{23} \\
0 & 0 & 1 & b_{31} & b_{32} & b_{33}
\end{array}
\right]
=
\left[ 
\begin{array}{c|c}
\mathbf{I} & \mathbf{A}^{-1} 
\end{array} 
\right]
\]
Diagonalization can be useful in cases where we need to find $\mathbf{A}^n$ for some large $n$.  Instead of multiplying $\mathbf A$ by itself $n$ times we can use diagonalization:
\[\mathbf A^n = (\mathbf C \mathbf D \mathbf C^{-1})^n = \mathbf C \mathbf D^n \mathbf C^{-1}\]
Where
\[\mathbf{D}^n = 
\begin{bmatrix}
d_{11}^n & 0 & 0 & \cdots & 0 \\
0 & d_{22}^n & 0 & \cdots & 0 \\
0 & 0 & d_{33}^n & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & d_{nn}^n
\end{bmatrix}\]
\newpage\noindent
\section*{4.9 Solving Systems by Elimination}
Simultaneous ordinary differential equations involve two or more equations that contain derivatives of two or more dependent variables (the unknown functions) \textbf{with respect to a single independent variable} can be solved by using differential operators to eliminate one of the dependent variables.\\\\
\underline{Ex:} Suppose we have the system of equations:
\begin{eqnarray*}
\frac{dx}{dt}&=&4x+7y\\
\frac{dy}{dt}&=&x-2y
\end{eqnarray*}
Rewriting with differential operators:
\begin{eqnarray*}
Dx -4x-7y&=&0\\
Dy-x+2y&=&0
\end{eqnarray*}
Now let's rearrange a little
\begin{eqnarray*}
(D-4)x-7y&=&0\\
-x+(D+2)y&=&0
\end{eqnarray*}
Now let's operate on the second equation by (D-4)
\begin{eqnarray*}
(D-4)x-7y&=&0\\
-(D-4)x+(D-4)(D+2)y&=&0
\end{eqnarray*}
and add
\begin{eqnarray*}
(D-4)(D+2)y-7y=D^2y-2Dy-15y=(D-5)(D+3)y=0
\end{eqnarray*}
Thus $$\boxed{y(t) = c_1e^{5t}+c_2e^{-3t}}$$
Now if we operated on the first equation by $(D+2)$ and multiplied the second equation by 7
\begin{eqnarray*}
(D+2)(D-4)x-7(D+2)y&=&0\\
-7x+7(D+2)y&=&0
\end{eqnarray*}
and added we would get
\begin{eqnarray*}
(D+2)(D-4)x-7x=D^2x-2Dx-15x=(D-5)(D+3)x=0\\
\end{eqnarray*}
so
$$\boxed{x(t) = c_3e^{5t}+c_4e^{-3t}}$$
Finally we substitute both solutions back into one of the original equations to the constants.
\newpage\noindent
\section*{8.1 Preliminary Theory - Linear Systems}
A \textbf{first order system} is made up of differential equations of the form:
\begin{eqnarray*}
\frac{dx_1}{dt}&=&g_1(t,x_1,x_2,\ldots,x_n)\\
\frac{dx_2}{dt}&=&g_2(t,x_1,x_2,\ldots,x_n)\\
&\vdots&\\
\frac{dx_n}{dt}&=&g_n(t,x_1,x_2,\ldots,x_n)
\end{eqnarray*}
If the functions $g_1,g_2\ldots g_n$ are linear in terms of the dependent variables $x_1,x_2,\ldots x_n$, then the system is called a \textbf{linear system}.
\begin{align*}
\frac{dx_1}{dt} &= a_{11}(t)x_1 + a_{12}(t)x_2 + \cdots + a_{1n}(t)x_n + f_1(t) \\
\frac{dx_2}{dt} &= a_{21}(t)x_1 + a_{22}(t)x_2 + \cdots + a_{2n}(t)x_n + f_2(t) \\
&\;\;\vdots \\
\frac{dx_n}{dt} &= a_{n1}(t)x_1 + a_{n2}(t)x_2 + \cdots + a_{nn}(t)x_n + f_n(t)
\end{align*}
This can be written in matrix form as:
\[\mathbf X'=\mathbf A\mathbf X+\mathbf F(t)\]
\[\begin{pmatrix}
x'_1\\x'_2\\\vdots\\x'_n
\end{pmatrix}
=
\begin{pmatrix}
a_{1,1}(t)&a_{1,2}(t)&\cdots& a_{1,n}(t)\\
a_{2,1}(t)&a_{2,2}(t)&\cdots& a_{2,n}(t)\\
\vdots&\vdots&\vdots\\
a_{n,1}(t)&a_{n,2}(t)&\cdots &a_{n,n}(t)\\
\end{pmatrix}
\begin{pmatrix}
x_1\\x_2\\\vdots\\x_n
\end{pmatrix}
+
\begin{pmatrix}
f_1(t)\\f_2(t)\\\vdots\\f_n(t)
\end{pmatrix}\]
Where the \textbf{solution vector} is
\[\mathbf{X}
=
\begin{pmatrix}
x_1(t)\\x_2(t)\\\vdots\\x_n(t)
\end{pmatrix}\]
whose entries are differentiable functions satisfying the system on the interval.\\\\
\underline{\textbf{Thm 8.1.1 Existence of a Unique Solution:}}\\
If the entries of $\mathbf A(t)$ and $\mathbf F(t)$ are continuous on an open interval $I$ containing $t_0$, then there exists a unique solution of the initial- value problem on the interval.
\begin{eqnarray*}
\mathbf X'&=&\mathbf A\mathbf X+\mathbf F(t)\\
\mathbf X(0)&=&\mathbf X_0
\end{eqnarray*}
\newpage\noindent
\underline{\textbf{Thm 8.1.2 Superposition Principle:}}\\
Let $\mathbf X_1, \mathbf X_2, . . . , \mathbf X_k$ be a set of solution vectors of the homogeneous system. Then the linear combination
$$\mathbf X=c_1\mathbf X_1 +c_2\mathbf X_2 +\cdots +c_k\mathbf X_k$$
where the $c_i$, $i = 1, 2, \ldots, k$ are arbitrary constants, is also a solution on the interval.\\\\
\underline{\textbf{Thm 8.1.3 Criterion for Linearly Independent Solutions:}}\\
For systems of first order differential equations, the Wronskian determinant is defined differently than for single differential equations.  Given $n$ solution vectors:
\[\mathbf{X}_1
=
\begin{pmatrix}
x_{11}(t)\\x_{21}(t)\\\vdots\\x_{n1}(t)
\end{pmatrix}
\mathbf{X}_2
=
\begin{pmatrix}
x_{12}(t)\\x_{22}(t)\\\vdots\\x_{n2}(t)
\end{pmatrix},\ldots,
\mathbf{X}_n
=
\begin{pmatrix}
x_{1n}(t)\\x_{2n}(t)\\\vdots\\x_{nn}(t)
\end{pmatrix}\]
The Wronskian determinant is defined as:
\[W(\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n) =
\begin{vmatrix}
x_{11} & x_{12} & \cdots & x_{1n} \\
x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nn}
\end{vmatrix} \]
\underline{Note:} This is just the determinant of the matrix formed by placing the solution vectors as columns.\\\\
Similarly, the solution vectors are linearly independent on the interval $I$ if and only if:
\[W(\mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_n) \neq 0 \quad \text{(for all t in $I$)}\]
\underline{Note:} Any set $\mathbf{X}_1,\mathbf{X}_2,\ldots,\mathbf{X}_n$ of $n$ linearly independent solution vectors to the homogeneous system on the interval $I$ is said to be a \textbf{fundamental set of solutions} on the interval.\\\\
\textbf{General Solution of a Nonhomogeneous System}\\
Similar to before, the general solution is:
\[\mathbf X = \mathbf X_c + \mathbf X_p\]
Where $\mathbf X_c$ is the general solution to the homogeneous system (\textbf{the complementary function}) and $\mathbf X_p$ is a particular solution to the nonhomogeneous system.
\newpage
\section*{8.2 Homogeneous Linear Systems}
Given a homogeneous linear system of the form:
\[\mathbf X' = \mathbf A \mathbf X\]
Where $\mathbf A$ is a constant matrix, we can solve for the general solution using eigenvalues and eigenvectors.\\\\
\textbf{Method:}\\
Find the eigenvalues and eigenvectors of $\mathbf A$ by solving:
\[\det(\mathbf A - \lambda \mathbf I) = 0\]
Let $\lambda_1, \lambda_2, \ldots, \lambda_n$ be the eigenvalues and $\mathbf v_1, \mathbf v_2, \ldots, \mathbf v_n$ be the corresponding eigenvectors. Then the general solution depends on the type of eigenvalues we have.\\\\
\textbf{Case 1: Real and Distinct Eigenvalues}\\
\[\boxed{\mathbf X(t) = k_{1}e^{\lambda_{1}t}\mathbf v_{1}+k_{2}e^{\lambda_{2}t}\mathbf v_{2}}\]
\textbf{Case 2: Complex Eigenvalues}\\
We know that complex eigenvalues and eigenvectors come in conjugate pairs, so we can just use the $+$ case and get the $-$ case by taking the complex conjugate.\\\\
Looking at the $+$ case, let $\lambda_1 = \alpha + i \beta$ be the eigenvalue and $\mathbf v_1 = \mathbf p + i \mathbf q$ be the eigenvector. Then the general solution is:
\[\boxed{\mathbf{X(t)}=[\mathbf{B_1}\cos(\beta t)-\mathbf{B_2}\sin(\beta t)]e^{\alpha t}+[\mathbf{B_2}\cos(\beta t)+\mathbf{B_1}\sin(\beta t)]e^{\alpha t}}\]
Where $\mathbf B_1$ and $\mathbf B_2$ are the real and imaginary parts of the eigenvector $\mathbf v_1$.
\[\mathbf{B_1}=Re(\mathbf{v_1})=\mathbf{p}\]
\[\mathbf{B_2}=Im(\mathbf{v_1})=\mathbf{q}\]
\underline{Note:} This formula comes from Euler's formula and separating the real and imaginary parts of the solution $ e^{\lambda_1 t} \mathbf v_1=e^{\alpha t}(\cos\beta t+i\sin\beta t) \mathbf v_1$.
\newpage\noindent
\textbf{Case 3: Real and Repeated Eigenvalues}\\
We will have two cases for eigenvalue $\lambda$:
\begin{enumerate}
    \item If 
    \[\mathbf A=\begin{pmatrix}a &0\\0 & a\end{pmatrix}\]
    Then we can choose \textbf{any} two linearly independent eigenvectors $\mathbf v_1$ and $\mathbf v_2$. We can keep it simple and choose:
    \[\mathbf v_1=\begin{pmatrix}1\\0\end{pmatrix}, \quad \mathbf v_2=\begin{pmatrix}0\\1\end{pmatrix}\]
    Thus, the general solution is:
    \[\boxed{\mathbf X(t) = k_1 e^{\lambda t} \mathbf v_1 + k_2 e^{\lambda t} \mathbf v_2}\]
    \item If not, we can use the initial conditions and set
$$\mathbf v_{0}= \begin{pmatrix}x(0)\\y(0) \end{pmatrix}$$
Then set
$$\mathbf v_{1} = (\mathbf A-\lambda \mathbf I)\mathbf v_{0}$$
And the solution becomes
$$\mathbf X = e^{\lambda t}\mathbf v_{0}+te^{\lambda t}\mathbf v_{1}$$
Nice thing about this method is it automatically solves for the initial conditions!    
\end{enumerate}
\section*{Phase Portraits}
By analyzing the eigenvalues and eigenvectors of the system, we can see the dynamics of the solutions across time.\\\\
\textbf{Real Eigenvalues}
\begin{itemize}
    \item Both eigenvalues positive: \textbf{Unstable Node} (all solutions move away from origin)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Unstable.png}
\end{figure}
    \item Both eigenvalues negative: \textbf{Stable Node} (all solutions move toward origin)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Stable.png}
\end{figure}
    \item Eigenvalues of opposite sign: \textbf{Saddle Point (Semi-Unstable)} (solutions move away from origin along one eigenvector and toward origin along the other eigenvector)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Saddle.png}
\end{figure}
\newpage\noindent
    \item Repeated eigenvalues: All solutions move toward or away from origin along eigenvector direction (depending on sign of eigenvalue). There are two cases for the two general solutions:
\begin{itemize}
    \item For a diagonal matrix, \textbf{all} vectors in $\mathbb{R}^2$ are eigenvectors, so solutions move directly toward or away from origin along straight lines in all directions.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Repeated Diagonal.png}
\end{figure}
    \item For a non-diagonal matrix, there is only \textbf{one} eigenvector, so solutions move toward or away from origin along the eigenvector direction, and other trajectories curve toward this line.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Repeated Non-Diagonal.png}
\end{figure}
\end{itemize}
\end{itemize}
The eigenvectors are straight line solutions that indicate the direction of the flow in the phase plane where all other trajectories are asymptotic to these lines (to satisfy uniqueness of solutions).
\newpage\noindent
\textbf{Complex Eigenvalues}
\begin{itemize}
    \item Real part positive: \textbf{Unstable Spiral} (solutions spiral away from origin)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Unstable Spiral.png}
\end{figure}
    \item Real part negative: \textbf{Stable Spiral} (solutions spiral toward origin)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Stable Spiral.png}
\end{figure}
\newpage
    \item Real part zero: \textbf{Center} (solutions circle around origin)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Center.png}
\end{figure}
\end{itemize}
By plugging in the initial conditions into the differential equation, we can see the direction of motion in terms of $dx/dt$ and $dy/dt$ at that point. Therefore, we can determine if the motion is clockwise or counterclockwise around the origin.
\section*{8.3 Non-Homogeneous Linear Systems}
To solve non-homogeneous linear systems of the form:
\[\mathbf X' = \mathbf A \mathbf X + \mathbf F(t)\]
We find the particular solution $\mathbf X_p$ using either \textbf{undetermined coefficients} or \textbf{variation of parameters}, and then add it to the complementary function $\mathbf X_c$ found from solving the homogeneous system.\\\\
\textbf{Undetermined Coefficients}
Similar to before, we guess the form of $\mathbf X_p$ based on the form of $\mathbf F(t)$.  However, now we must guess a vector function.\\\\
\underline{Ex:} Given the system:
 \begin{eqnarray*}
 \frac{dx}{dt} &=& 2x+3y-7\\
 \frac{dy}{dt} &=& -x-2y+5\\
 \end{eqnarray*}
 This can be rearranged as:










\end{document}